{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33aa946a-389f-4cb5-b726-252de82cc47b",
   "metadata": {},
   "source": [
    "# Comparing Neural Networks with and without Regularization on MNIST\n",
    "\n",
    "## Introduction\n",
    "We will compare the performance of a simple neural network on the MNIST dataset, with and without the application of L2 regularization. Regularization is a technique used to improve the generalization of machine learning models, helping them perform better on unseen data by preventing overfitting.\n",
    "\n",
    "We will:\n",
    "- Train a neural network without regularization.\n",
    "- Train a neural network with L2 regularization.\n",
    "- Compare the results in terms of accuracy and loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d897c9f6-fa15-404d-8388-a3fe2b1e8afc",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae32b844-c4eb-4728-91da-b7c4bc5169b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the images to [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Show the shape of the data\n",
    "print(f'Training data shape: {x_train.shape}')\n",
    "print(f'Test data shape: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f8b8e-e655-4158-806a-84b6c507d7ce",
   "metadata": {},
   "source": [
    "## Defining the Neural Network Models\n",
    "We will create two neural network models:\n",
    "- **Model 1**: Without regularization.\n",
    "- **Model 2**: With L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d7678f-1fe6-47f0-8a8c-e3501f9d7868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define a function to create the neural network model\n",
    "def create_model(with_regularization=False):\n",
    "    if with_regularization:\n",
    "        regularizer = l2(0.001)  # L2 regularization with lambda = 0.001\n",
    "    else:\n",
    "        regularizer = None\n",
    "\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=regularizer),\n",
    "        Dense(64, activation='relu', kernel_regularizer=regularizer),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create models\n",
    "model_no_reg = create_model(with_regularization=False)\n",
    "model_with_reg = create_model(with_regularization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3eb334-251b-4897-b581-49a87815def6",
   "metadata": {},
   "source": [
    "## Training the Models\n",
    "Next, we will compile and train both models. We will use the Adam optimizer and categorical crossentropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520efa01-27c2-4bc9-99ca-6c769a4bdefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the models\n",
    "model_no_reg.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_with_reg.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model without regularization\n",
    "history_no_reg = model_no_reg.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Train the model with L2 regularization\n",
    "history_with_reg = model_with_reg.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7de63-af76-4e63-8e86-5ab93d2fff68",
   "metadata": {},
   "source": [
    "## Evaluating the Models\n",
    "After training, we will evaluate both models on the test set to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83d2c5c-c247-4e83-bb72-b329c8019748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models on the test set\n",
    "test_loss_no_reg, test_acc_no_reg = model_no_reg.evaluate(x_test, y_test, verbose=0)\n",
    "test_loss_with_reg, test_acc_with_reg = model_with_reg.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Test accuracy without regularization: {test_acc_no_reg:.4f}\")\n",
    "print(f\"Test accuracy with L2 regularization: {test_acc_with_reg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef88bb16-f76e-4760-a1ae-48db2a71dc6c",
   "metadata": {},
   "source": [
    "## Visualizing Results\n",
    "We will plot the training and validation accuracy and loss for both models to observe the effects of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cfca92-3e30-44ea-8df9-441cd33c7f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy and loss\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Accuracy plots\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_no_reg.history['accuracy'], label='Train No Reg')\n",
    "plt.plot(history_no_reg.history['val_accuracy'], label='Val No Reg')\n",
    "plt.plot(history_with_reg.history['accuracy'], label='Train L2 Reg')\n",
    "plt.plot(history_with_reg.history['val_accuracy'], label='Val L2 Reg')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Loss plots\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_no_reg.history['loss'], label='Train No Reg')\n",
    "plt.plot(history_no_reg.history['val_loss'], label='Val No Reg')\n",
    "plt.plot(history_with_reg.history['loss'], label='Train L2 Reg')\n",
    "plt.plot(history_with_reg.history['val_loss'], label='Val L2 Reg')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dda8c8e-64ab-4282-b37e-30caeb693da5",
   "metadata": {},
   "source": [
    "![Alt text](regularization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a2b15-820f-448f-8343-5381426913f3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "From the results, we can observe that the model with L2 regularization shows reduced validation loss compared to the model without regularization, showing better generalization and recognition to new or unseen data. Regularization also showed a slight decrease in accuracy on the trained models due to the restraint on overfitting which allowed for the better recognition on new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
