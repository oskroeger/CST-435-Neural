{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### load all the libraries ###"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2c0682a5d563a17"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m confusion_matrix, roc_curve, auc\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_dataset\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Variables to set the number of epochs and samples\n",
    "num_epochs = 10\n",
    "num_samples = 100  # set this to -1 to use all data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T19:56:32.316060Z",
     "start_time": "2023-10-11T19:56:32.299664Z"
    }
   },
   "id": "4576e89f73c4dfe0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### load the dataset and the model tokenizer ###"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd761ce092191106"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 1: Load dataset and model tokenizer\n",
    "dataset = load_dataset('imdb')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6071385fed8e1189"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### create a plot to see the distribution of the positive and negative classes ###"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c58047696700adf4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data Exploration\n",
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "sns.countplot(x='label', data=train_df)\n",
    "plt.title('Class distribution')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1353a8e1e70ff11e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### preprocess our dataset by tokenizing the texts. We use BERT’s tokenizer, which will convert the text into tokens that correspond to BERT’s vocabulary ###"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3ea3f9621a6b692"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 2: Preprocess the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcbad849c5a40bdf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# prepare our training and evaluation datasets. Remember, if you want to use all the data, you can set the num_samples variable to -1 ###"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9ca7c67c692a669"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if num_samples == -1:\n",
    "    small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "    small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)\n",
    "else:\n",
    "    small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(num_samples)) \n",
    "    small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(num_samples)) \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "997d7bb5c9da3ee5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### load the pre-trained BERT model. We’ll use the AutoModelForSequenceClassification class, a BERT model designed for classification tasks ###"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f52d35802407bcd6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### *use the ‘bert-base-uncased’ version of BERT, which is trained on lower-case English text* ###"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2ccca7f9fb18dd9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 3: Load pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57df9797fbd20332"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### define our training arguments and create a Trainer instance to train our model. ###"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7125357a05d4815"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 4: Define training arguments\n",
    "training_args = TrainingArguments(\"test_trainer\", evaluation_strategy=\"epoch\", no_cuda=True, num_train_epochs=num_epochs)\n",
    "\n",
    "# Step 5: Create Trainer instance and train\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f08de11d96a5b743"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Interpreting Results ###\n",
    "Having trained our model, let’s evaluate it. We’ll calculate the confusion matrix and the ROC curve to understand how well our model performs.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae9dded99d5064c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 6: Evaluation\n",
    "predictions = trainer.predict(small_eval_dataset)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(small_eval_dataset['label'], predictions.predictions.argmax(-1))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(small_eval_dataset['label'], predictions.predictions[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(1.618 * 5, 5))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d55734817556fa9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The confusion matrix gives a detailed breakdown of how our predictions measure up to the actual labels, while the ROC curve shows us the trade-off between the true positive rate (sensitivity) and the false positive rate (1 — specificity) at various threshold settings ###\n",
    "\n",
    "### see our model in action, use it to infer the sentiment of a sample text ###"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d271f621f7f6dfe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 7: Inference on a new sample\n",
    "sample_text = \"This is a fantastic movie. I really enjoyed it.\"\n",
    "sample_inputs = tokenizer(sample_text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to device (if GPU available)\n",
    "sample_inputs.to(training_args.device)\n",
    "\n",
    "# Make prediction\n",
    "predictions = model(**sample_inputs)\n",
    "predicted_class = predictions.logits.argmax(-1).item()\n",
    "\n",
    "if predicted_class == 1:\n",
    "    print(\"Positive sentiment\")\n",
    "else:\n",
    "    print(\"Negative sentiment\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ee8298a763e2098"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
