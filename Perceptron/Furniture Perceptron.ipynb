{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8afcc95-a4d7-4d1f-90ed-3bafe40eb614",
   "metadata": {},
   "source": [
    "### Owen Kroeger and Teddy Coon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da76b57d-171a-49cd-901f-7bf47f26957e",
   "metadata": {},
   "source": [
    "# Perceptron Optimization for Furniture Placement in a Room\n",
    "\n",
    "This notebook explores how a simple perceptron can be used to find the optimal \n",
    "placement for furniture within a room.\n",
    "\n",
    "### Videos:\n",
    "- **Teddy**: https://www.loom.com/share/fb173ad276e4413c881b2ac63e92fa06\n",
    "- **Owen**: https://www.loom.com/share/1e3ffb0dde9349ab8e4f856b9c35d716?sid=3fd6e910-b5d2-4683-a523-bab3d195cf89"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf26cf-56c9-4474-8559-2c9e39d67809",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "The objective is to optimize the placement of furniture in a room using a perceptron model. \n",
    "The room is represented as a 10x10 grid where each cell can either be optimal (1) or not optimal (0). \n",
    "We aim to train the perceptron to learn these optimal placements based on given grid data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c963bc0b-db02-4eb6-870c-694d48c517d6",
   "metadata": {},
   "source": [
    "## Algorithm of the Solution\n",
    "\n",
    "The solution uses a perceptron model with the following components:\n",
    "- **Sigmoid Activation Function**: Used during training to calculate predictions.\n",
    "- **Step Function**: Used for final binary decisions.\n",
    "- **Training**: The perceptron is trained using a simple feedforward approach with error correction.\n",
    "- **Visualization**: Intermediate results and errors are visualized using heatmaps and plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efef4b-72e6-435e-ad82-95e70b439142",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries\n",
    "\n",
    "We will use NumPy for numerical operations and Matplotlib for plotting and visualizing \n",
    "the perceptron's performance. The following libraries are essential for the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50b628-0186-4a34-a9ba-e82c1f66e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.gridspec import GridSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee0ebe-9549-4147-823c-7bd321a7e336",
   "metadata": {},
   "source": [
    "## Define Activation Functions\n",
    "\n",
    "The perceptron uses two types of activation functions:\n",
    "- **Sigmoid Activation Function**: Used during the training phase to calculate continuous predictions and update weights.\n",
    "- **Binary Step Function**: Used for making final binary predictions (optimal or not optimal) after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b35fe7-1d51-4715-84cc-628a06d4eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function for training\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Binary activation function for final predictions\n",
    "def activation_function(x):\n",
    "    return 1 if x >= 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910e0f0-fef5-42a9-8ba8-5cc4b7a8acfa",
   "metadata": {},
   "source": [
    "## Perceptron Training Function\n",
    "\n",
    "The perceptron is trained using a feedforward approach with error correction. \n",
    "The weights are adjusted based on the error between the predicted and actual outputs. \n",
    "The function captures intermediate results to visualize how the perceptron learns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640b192-8b4b-4800-b2b4-ba0d00bf3a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron training function with intermediate heatmap captures and error tracking\n",
    "def train_perceptron(data, labels, epochs=200, learning_rate=0.2):\n",
    "    # Initialize weights and bias with small random values for more nuanced updates\n",
    "    weights = np.random.randn(data.shape[1]) * 0.01\n",
    "    bias = 0.0\n",
    "    intermediate_weighted_sums = []  # To store intermediate weighted sums\n",
    "    errors = []  # To track error over epochs\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_error = 0\n",
    "        for i in range(len(data)):\n",
    "            x = data[i]\n",
    "            y = labels[i]\n",
    "\n",
    "            # Calculate the weighted sum and use sigmoid for training\n",
    "            weighted_sum = np.dot(weights, x) + bias\n",
    "            prediction = sigmoid(weighted_sum)  # Use sigmoid for more varied weight updates\n",
    "\n",
    "            # Calculate the gradient with respect to the sigmoid output\n",
    "            error = y - prediction\n",
    "            total_error += error ** 2  # Accumulate squared error\n",
    "            weights += learning_rate * error * x * prediction * (1 - prediction)  # Derivative of sigmoid\n",
    "            bias += learning_rate * error * prediction * (1 - prediction)\n",
    "\n",
    "        # Store total error for the epoch\n",
    "        errors.append(total_error)\n",
    "\n",
    "        # Capture weighted sums at the start, halfway, and at the end\n",
    "        if epoch == 0 or epoch == epochs // 2 or epoch == epochs - 1:\n",
    "            current_weighted_sums = [np.dot(weights, data[i]) + bias for i in range(len(data))]\n",
    "            intermediate_weighted_sums.append(np.array(current_weighted_sums).reshape(10, 10))\n",
    "\n",
    "    return weights, bias, intermediate_weighted_sums, errors\n",
    "\n",
    "# Manually inputting the grid values into training_data and corresponding labels\n",
    "training_data = np.eye(100)  # 100 vectors, each one representing a grid cell (one-hot encoded)\n",
    "\n",
    "# Labels manually inputted based on the grid data (flattened from the grid)\n",
    "labels = [\n",
    "    1, 1, 0, 0, 1, 1, 0, 0, 1, 1,  # Row 1\n",
    "    1, 0, 0, 0, 0, 0, 0, 0, 0, 1,  # Row 2\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  # Row 3\n",
    "    0, 0, 0, 1, 1, 1, 1, 0, 0, 0,  # Row 4\n",
    "    1, 0, 0, 1, 1, 1, 1, 0, 0, 1,  # Row 5\n",
    "    1, 0, 0, 1, 1, 1, 1, 0, 0, 1,  # Row 6\n",
    "    0, 0, 0, 1, 1, 1, 1, 0, 0, 0,  # Row 7\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  # Row 8\n",
    "    1, 0, 0, 0, 0, 0, 0, 0, 0, 1,  # Row 9\n",
    "    1, 1, 0, 0, 1, 1, 0, 0, 1, 1  # Row 10\n",
    "]\n",
    "\n",
    "# Train the perceptron using the sigmoid function for training and capture intermediate states and errors\n",
    "weights, bias, intermediate_weighted_sums, errors = train_perceptron(training_data, labels, epochs=200,\n",
    "                                                                     learning_rate=0.2)\n",
    "\n",
    "# Final testing to capture the final predictions\n",
    "weighted_sums = [np.dot(weights, training_data[i]) + bias for i in range(100)]\n",
    "binary_predictions = [activation_function(ws) for ws in weighted_sums]\n",
    "\n",
    "# Reshape weighted sums, binary predictions, and labels to 10x10 grids for visualization\n",
    "grid_weighted_sums = np.array(weighted_sums).reshape(10, 10)\n",
    "grid_predictions = np.array(binary_predictions).reshape(10, 10)\n",
    "grid_expected = np.array(labels).reshape(10, 10)\n",
    "\n",
    "# Use symmetric normalization to enhance contrast evenly for both reds and blues\n",
    "norm = Normalize(vmin=-np.max(np.abs(grid_weighted_sums)), vmax=np.max(np.abs(grid_weighted_sums)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff4301-bf09-44f9-97ac-a7e084986465",
   "metadata": {},
   "source": [
    "## Output and Analysis\n",
    "\n",
    "### Intermediate Results: Heatmaps of Weighted Sums\n",
    "\n",
    "The following heatmaps represent the perceptron's confidence (weighted sums) at different \n",
    "stages of training: the start, midway, and end. These visualizations help us understand \n",
    "how the model’s predictions evolve over time.\n",
    "\n",
    "![Heatmap Start](images/beggining_heatmap.png)\n",
    "![Heatmap Midway](images/midway_heatmap.png)\n",
    "![Heatmap Final](images/ending_heatmap.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afb8aec-947a-4ea6-807b-b2c5078af83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize intermediate heatmaps\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "gs = GridSpec(2, 3, figure=fig)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "im1 = ax1.imshow(intermediate_weighted_sums[0], cmap='RdBu_r', interpolation='nearest', norm=norm)\n",
    "ax1.set_title('Heatmap at Start of Training')\n",
    "ax1.set_xlabel('Columns')\n",
    "ax1.set_ylabel('Rows')\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "im2 = ax2.imshow(intermediate_weighted_sums[1], cmap='RdBu_r', interpolation='nearest', norm=norm)\n",
    "ax2.set_title('Heatmap Midway Through Training')\n",
    "ax2.set_xlabel('Columns')\n",
    "ax2.set_ylabel('Rows')\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "im3 = ax3.imshow(intermediate_weighted_sums[2], cmap='RdBu_r', interpolation='nearest', norm=norm)\n",
    "ax3.set_title('Final Heatmap of Weighted Sums')\n",
    "ax3.set_xlabel('Columns')\n",
    "ax3.set_ylabel('Rows')\n",
    "\n",
    "# Add a colorbar to the right of the heatmaps\n",
    "cbar = fig.colorbar(im3, ax=[ax1, ax2, ax3], fraction=0.015, pad=0.04, orientation='vertical')\n",
    "cbar.set_label('Weighted Sum (Gradient of Optimality)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6558f5a-1659-45d2-888f-628dc5831c93",
   "metadata": {},
   "source": [
    "### Final Outputs: Binary Decisions and Error Reduction\n",
    "\n",
    "Below, we see the perceptron's final binary decisions (optimal vs. not optimal) compared against the \n",
    "expected values. Additionally, the error plot illustrates the reduction of training error over epochs, \n",
    "showing how the model improves with training.\n",
    "\n",
    "![Heatmap Start](images/finalvalues.png)\n",
    "![Heatmap Start](images/error_reduction.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5e909-7c66-448f-b871-ee1170b9d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final binary decisions, expected values, and error plot\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "gs = GridSpec(2, 3, figure=fig)\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "im4 = ax4.imshow(grid_predictions, cmap='gray', interpolation='nearest')\n",
    "ax4.set_title('Final Binary Decisions (1 = Optimal, 0 = Not Optimal)')\n",
    "ax4.set_xlabel('Columns')\n",
    "ax4.set_ylabel('Rows')\n",
    "\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "im5 = ax5.imshow(grid_expected, cmap='gray', interpolation='nearest')\n",
    "ax5.set_title('Expected Values (1 = Optimal, 0 = Not Optimal)')\n",
    "ax5.set_xlabel('Columns')\n",
    "ax5.set_ylabel('Rows')\n",
    "\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "ax6.plot(errors, label='Training Error')\n",
    "ax6.set_title('Error Reduction Over Epochs')\n",
    "ax6.set_xlabel('Epoch')\n",
    "ax6.set_ylabel('Total Squared Error')\n",
    "ax6.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f069666-dcf4-44ee-9d4d-f5f4cb32c4d9",
   "metadata": {},
   "source": [
    "### Analysis of the Findings\n",
    "\n",
    "- **Intermediate Heatmaps**: The heatmaps show a progression of the perceptron’s confidence from\n",
    "random guesses at the start to more refined predictions by the end. Midway heatmaps provide\n",
    "insight into the learning process.\n",
    "- **Final Binary Decisions**: Comparing the final binary decisions with the expected values\n",
    "reveals areas where the perceptron correctly identifies optimal placements and where it fails.\n",
    "- **Error Plot**: The error plot indicates a clear reduction in error over epochs,\n",
    "demonstrating the perceptron's learning capability. A consistent decrease suggests effective\n",
    "training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
